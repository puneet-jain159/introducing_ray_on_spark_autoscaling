{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b1d41fc-1d88-42c9-a2f4-2fcefb7f238a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Autoscaling Ray on Databricks and Apache Spark\n",
    "\n",
    "With the release of **Ray 2.8.0**, we have enabled Ray auto-scaling with Ray on Databricks and Apache Spark. Below, we showcase the functionality by going through an example of hyper-parameter tuning for a deep learning model on the CIFAR dataset.\n",
    "\n",
    "Ray Auto-scaling works with **DBR runtime 14+**, and the code has been tested with the following cluster configurations:\n",
    "\n",
    "**Azure**: Driver NC6s_v3 and autoscaling with 4 worker nodes NC6s_v3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df66e3bc-787a-40dd-bd63-c8aca1369695",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Install the Ray library and any other python Dependencies\n",
    "Once specified you do not need to respecify the libraries during Ray initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "211bcd13-82dc-4a8c-9796-ee679e9a5fa4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-feature-store 0.14.1 requires pyspark<4,>=3.1.2, which is not installed.\ntensorflow 2.11.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.0 which is incompatible.\ntensorboard 2.11.0 requires protobuf<4,>=3.9.2, but you have protobuf 4.25.0 which is incompatible.\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install ray['default,tune'] >=2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f3f9d7-f311-470e-b5c5-40e982071a09",
     "showTitle": true,
     "title": "Optional: to  restart the python interpretator if the custom installation requires restart"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "684d2759-fb1d-4a40-9364-4ff984e078d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start the ray cluster \n",
    "Use the Ray on spark API's to start the cluster refer to the [here](https://docs.ray.io/en/latest/cluster/vms/user-guides/community/spark.html?highlight=ray.util.spark#ray-on-spark-apis)  for more details on the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53510962-cf01-44e3-a4c4-e2bf766240d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 21:18:42,105\tINFO cluster_init.py:528 -- Ray head hostname 10.139.64.118, port 9124\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-06 21:18:44,307\tINFO usage_lib.py:416 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n2023-11-06 21:18:44,307\tINFO scripts.py:744 -- \u001B[37mLocal node IP\u001B[39m: \u001B[1m10.139.64.118\u001B[22m\n2023-11-06 21:18:46,380\tSUCC scripts.py:781 -- \u001B[32m--------------------\u001B[39m\n2023-11-06 21:18:46,380\tSUCC scripts.py:782 -- \u001B[32mRay runtime started.\u001B[39m\n2023-11-06 21:18:46,380\tSUCC scripts.py:783 -- \u001B[32m--------------------\u001B[39m\n2023-11-06 21:18:46,380\tINFO scripts.py:785 -- \u001B[36mNext steps\u001B[39m\n2023-11-06 21:18:46,380\tINFO scripts.py:788 -- To add another node to this Ray cluster, run\n2023-11-06 21:18:46,380\tINFO scripts.py:791 -- \u001B[1m  ray start --address='10.139.64.118:9124'\u001B[22m\n2023-11-06 21:18:46,380\tINFO scripts.py:800 -- To connect to this Ray cluster:\n2023-11-06 21:18:46,380\tINFO scripts.py:802 -- \u001B[35mimport\u001B[39m\u001B[26m ray\n2023-11-06 21:18:46,380\tINFO scripts.py:803 -- ray\u001B[35m.\u001B[39m\u001B[26minit(_node_ip_address\u001B[35m=\u001B[39m\u001B[26m\u001B[33m'10.139.64.118'\u001B[39m\u001B[26m)\n2023-11-06 21:18:46,380\tINFO scripts.py:815 -- To submit a Ray job using the Ray Jobs CLI:\n2023-11-06 21:18:46,380\tINFO scripts.py:816 -- \u001B[1m  RAY_ADDRESS='http://10.139.64.118:9137' ray job submit --working-dir . -- python my_script.py\u001B[22m\n2023-11-06 21:18:46,380\tINFO scripts.py:825 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html \n2023-11-06 21:18:46,380\tINFO scripts.py:829 -- for more information on submitting Ray jobs to the Ray cluster.\n2023-11-06 21:18:46,380\tINFO scripts.py:834 -- To terminate the Ray runtime, run\n2023-11-06 21:18:46,381\tINFO scripts.py:835 -- \u001B[1m  ray stop\u001B[22m\n2023-11-06 21:18:46,381\tINFO scripts.py:838 -- To view the status of the cluster, use\n2023-11-06 21:18:46,381\tINFO scripts.py:839 --   \u001B[1mray status\u001B[22m\u001B[26m\n2023-11-06 21:18:46,381\tINFO scripts.py:843 -- To monitor and debug Ray, view the dashboard at \n2023-11-06 21:18:46,381\tINFO scripts.py:844 --   \u001B[1m10.139.64.118:9137\u001B[22m\u001B[26m\n2023-11-06 21:18:46,381\tINFO scripts.py:851 -- \u001B[4mIf connection to the dashboard fails, check your firewall settings and network configuration.\u001B[24m\n2023-11-06 21:18:46,381\tINFO scripts.py:952 -- \u001B[36m\u001B[1m--block\u001B[22m\u001B[39m\n2023-11-06 21:18:46,381\tINFO scripts.py:953 -- This command will now block forever until terminated by a signal.\n2023-11-06 21:18:46,381\tINFO scripts.py:956 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 21:19:02,138\tINFO cluster_init.py:640 -- Ray head node started.\n2023-11-06 21:19:02,143\tINFO worker.py:1489 -- Connecting to existing Ray cluster at address: 10.139.64.118:9124...\n2023-11-06 21:19:02,153\tINFO worker.py:1664 -- Connected to Ray cluster. View the dashboard at \u001B[1m\u001B[32m10.139.64.118:9137 \u001B[39m\u001B[22m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To monitor and debug Ray from Databricks, view the dashboard at \n https://dbc-dp-984752964297111.cloud.databricks.com/driver-proxy/o/984752964297111/1023-112611-gamx0lyy/9137/\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "      <div style=\"margin-bottom: 16px\">\n",
       "          <a href=\"/driver-proxy/o/984752964297111/1023-112611-gamx0lyy/9137/\">\n",
       "              Open Ray Cluster Dashboard in a new tab\n",
       "          </a>\n",
       "      </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "\n",
    "\n",
    "num_cpu_cores_per_worker = 4 # total cpu's present in each node\n",
    "num_cpus_head_node = 4\n",
    "num_gpu_per_worker = 1\n",
    "num_gpus_head_node = 1\n",
    "\n",
    "ray_conf = setup_ray_cluster(\n",
    "  num_worker_nodes= 4,#this should be set max number of nodes the cluster is allowed to auto-scale\n",
    "  num_cpus_head_node= num_cpus_head_node, #this should be set cores used in the driver node used for jobs\n",
    "  num_gpus_head_node= num_gpus_head_node, #this only should be set for GPU enabled cluster \n",
    "  num_cpus_per_node=num_cpu_cores_per_worker, #this should be set cores added from each worker node \n",
    "  num_gpus_per_node=num_gpu_per_worker,#this should be set gpus added from each worker node \n",
    "  autoscale = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b241d9f3-915a-4d3e-88a1-9ad8e5effccf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Incase you want to restart the cluster use `shutdown_ray_cluster` this will not restart the interpretor or REPL\n",
    "# shutdown_ray_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44256d3f-ae9e-4afe-8544-789dc90f9cc1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f4f7522-9c45-4dbc-9e80-3b79f2545009",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.train import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d1b36d-a142-4128-a6eb-9baf6b376764",
     "showTitle": true,
     "title": "Helper Function to Load the CIFAR dataset ."
    }
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"./data\"):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # We add FileLock here because multiple workers will want to\n",
    "    # download data, and this may cause overwrites since\n",
    "    # DataLoader is not threadsafe.\n",
    "    with FileLock(os.path.expanduser(\"~/.data.lock\")):\n",
    "        trainset = torchvision.datasets.CIFAR10(\n",
    "            root=data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(\n",
    "            root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc7352af-3907-415f-96f9-d2869664be4f",
     "showTitle": true,
     "title": "Define the Torch Model with Regularization as the hyper-parameter tuning variable"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, l1=120, l2=84):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        self.fc3 = nn.Linear(l2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cb42944-3f2f-4496-a293-863870a49019",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##The Train function\n",
    "Now it gets interesting, because we introduce some changes to the example from the [PyTorch documentation](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).\n",
    "\n",
    "The full code example looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3f717a8-6851-4334-9850-c3779401c587",
     "showTitle": true,
     "title": "Define the Function which takes a config and runs the Training loop for the torch model"
    }
   },
   "outputs": [],
   "source": [
    "def train_cifar(config,loc):\n",
    "    \n",
    "    print(\"num_cpus:\",int(train.get_context().get_trial_resources().head_cpus))\n",
    "    torch.set_num_threads(int(train.get_context().get_trial_resources().head_cpus))\n",
    "    net = Net(config[\"l1\"], config[\"l2\"])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    # To restore a checkpoint, use `train.get_checkpoint()`.\n",
    "    loaded_checkpoint = train.get_checkpoint()\n",
    "    if loaded_checkpoint:\n",
    "        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "           model_state, optimizer_state = torch.load(os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    trainset, testset = load_data(data_dir)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "\n",
    "    for epoch in range(config['max_epoch']):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        # Here we save a checkpoint. It is automatically registered with\n",
    "        # Ray Tune and can be accessed through `train.get_checkpoint()`\n",
    "        # API in future iterations.\n",
    "        os.makedirs(f\"{loc}/mymodel\", exist_ok=True)\n",
    "        torch.save(\n",
    "            (net.state_dict(), optimizer.state_dict()), f\"{loc}/mymodel/checkpoint.pt\")\n",
    "        checkpoint = Checkpoint.from_directory(f\"{loc}/mymodel/\")\n",
    "        train.report({\"loss\": (val_loss / val_steps),\"try_gpu\" : False, \"accuracy\": correct / total}, checkpoint=checkpoint)\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c7edf2-0e40-490a-ab31-7862e9ee5b1b",
     "showTitle": true,
     "title": "Define the Ray Tuner API"
    }
   },
   "outputs": [],
   "source": [
    "def main(num_samples=10, max_num_epochs=10,\n",
    "         grace_period=5,cpus_per_trial=1, \n",
    "         gpus_per_trial=0 , loc = '/dbfs/pj/ray/'):\n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16]),\n",
    "        \"max_epoch\":20\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=config['max_epoch'],\n",
    "        grace_period=5,\n",
    "        reduction_factor=2)\n",
    "    \n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_cifar,loc = loc),\n",
    "            resources={\"cpu\": cpus_per_trial, \"gpu\":gpus_per_trial }\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "        ),\n",
    "        run_config=train.RunConfig(\n",
    "        storage_path=os.path.expanduser(loc),\n",
    "        name=\"tune_checkpointing_location\",\n",
    "    ),\n",
    "        param_space=config,\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    \n",
    "    best_result = results.get_best_result(\"loss\", \"min\")\n",
    "\n",
    "    print(\"Best trial config: {}\".format(best_result.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_result.metrics[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_result.metrics[\"accuracy\"]))\n",
    "\n",
    "    test_best_model(best_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68979682-dccf-4cc6-ba6e-43cf264d3973",
     "showTitle": true,
     "title": "Define function to test the best trail:"
    }
   },
   "outputs": [],
   "source": [
    "def test_best_model(best_result):\n",
    "    best_trained_model = Net(best_result.config[\"l1\"], best_result.config[\"l2\"])\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "\n",
    "    model_state, optimizer_state = torch.load(checkpoint_path)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = best_trained_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    print(\"Best trial test set accuracy: {}\".format(correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b81217a-6317-46b1-8c8b-1061390729d4",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Run a CPU only Trial\n",
    "main(num_samples=8, max_num_epochs=10,grace_period=5,cpus_per_trial=3, gpus_per_trial=0 , loc = '/dbfs/pj/ray/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8983241-ba85-458a-aa57-f002df4253fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 21:19:06,551\tINFO worker.py:1354 -- Using address 10.139.64.118:9124 set in the environment variable RAY_ADDRESS\n2023-11-06 21:19:06,551\tINFO worker.py:1489 -- Connecting to existing Ray cluster at address: 10.139.64.118:9124...\n2023-11-06 21:19:06,558\tINFO worker.py:1664 -- Connected to Ray cluster. View the dashboard at \u001B[1m\u001B[32m10.139.64.118:9137 \u001B[39m\u001B[22m\n2023-11-06 21:19:06,816\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n2023-11-06 21:19:06,819\tINFO tune.py:595 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+\n| Configuration for experiment     tune_checkpointing_location   |\n+----------------------------------------------------------------+\n| Search algorithm                 BasicVariantGenerator         |\n| Scheduler                        AsyncHyperBandScheduler       |\n| Number of trials                 8                             |\n+----------------------------------------------------------------+\n\nView detailed results here: /dbfs/pj/ray/tune_checkpointing_location\nTo visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/tune_checkpointing_location`\n\nTrial status: 8 PENDING\nCurrent time: 2023-11-06 21:19:07. Total running time: 0s\nLogical resource usage: 0/4 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:V100)\n+----------------------------------------------------------------+\n| Trial name                status             lr     batch_size |\n+----------------------------------------------------------------+\n| train_cifar_1e5c3_00000   PENDING    0.00119966              2 |\n| train_cifar_1e5c3_00001   PENDING    0.00170084             16 |\n| train_cifar_1e5c3_00002   PENDING    0.0126673               4 |\n| train_cifar_1e5c3_00003   PENDING    0.00612462              8 |\n| train_cifar_1e5c3_00004   PENDING    0.0312312               4 |\n| train_cifar_1e5c3_00005   PENDING    0.00117903              4 |\n| train_cifar_1e5c3_00006   PENDING    0.0627759              16 |\n| train_cifar_1e5c3_00007   PENDING    0.013142                8 |\n+----------------------------------------------------------------+\n\u001B[36m(autoscaler +33s)\u001B[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n\u001B[36m(autoscaler +33s)\u001B[0m Adding 2 node(s) of type ray.worker.\n\nTrial train_cifar_1e5c3_00000 started with configuration:\n+-------------------------------------------------+\n| Trial train_cifar_1e5c3_00000 config            |\n+-------------------------------------------------+\n| batch_size                                    2 |\n| l1                                            8 |\n| l2                                          256 |\n| lr                                       0.0012 |\n| max_epoch                                    20 |\n+-------------------------------------------------+\n\nTrial train_cifar_1e5c3_00001 started with configuration:\n+-------------------------------------------------+\n| Trial train_cifar_1e5c3_00001 config            |\n+-------------------------------------------------+\n| batch_size                                   16 |\n| l1                                            4 |\n| l2                                            8 |\n| lr                                       0.0017 |\n| max_epoch                                    20 |\n+-------------------------------------------------+\n\u001B[36m(train_cifar pid=3952)\u001B[0m num_cpus: 1\n\u001B[36m(train_cifar pid=3953)\u001B[0m Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m \r  0%|          | 0/170498071 [00:00<?, ?it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r  1%|          | 884736/170498071 [00:00<00:20, 8256869.30it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r  6%|▌         | 10158080/170498071 [00:00<00:02, 56489307.72it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 11%|█▏        | 19529728/170498071 [00:00<00:02, 73158467.09it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 18%|█▊        | 30539776/170498071 [00:00<00:01, 87585855.10it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 23%|██▎       | 39878656/170498071 [00:00<00:01, 89643204.70it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 30%|██▉       | 51019776/170498071 [00:00<00:01, 96988530.14it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 36%|███▌      | 60784640/170498071 [00:00<00:01, 95656279.61it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 42%|████▏     | 72220672/170498071 [00:00<00:00, 101461161.60it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 48%|████▊     | 82411520/170498071 [00:00<00:00, 98252274.26it/s] \n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 55%|█████▌    | 94044160/170498071 [00:01<00:00, 103611323.93it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 61%|██████▏   | 104464384/170498071 [00:01<00:00, 99632824.27it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 68%|██████▊   | 115867648/170498071 [00:01<00:00, 103822209.04it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 74%|███████▍  | 126320640/170498071 [00:01<00:00, 100749542.64it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 81%|████████  | 137527296/170498071 [00:01<00:00, 103921436.69it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 87%|████████▋ | 147980288/170498071 [00:01<00:00, 101331156.33it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 93%|█████████▎| 158924800/170498071 [00:01<00:00, 103595182.20it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 99%|█████████▉| 169345024/170498071 [00:01<00:00, 101664525.20it/s]\r100%|██████████| 170498071/170498071 [00:01<00:00, 95982674.65it/s] \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Extracting /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/data/cifar-10-python.tar.gz to /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/data\n\u001B[36m(autoscaler +39s)\u001B[0m Resized to 8 CPUs, 2 GPUs.\n\u001B[36m(train_cifar pid=3953)\u001B[0m Files already downloaded and verified\n\u001B[36m(train_cifar pid=3953)\u001B[0m num_cpus: 1\n\u001B[36m(autoscaler +41s)\u001B[0m Adding 1 node(s) of type ray.worker.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m /databricks/python/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n\u001B[36m(train_cifar pid=3953)\u001B[0m   warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00000_0_batch_size=2,lr=0.0012_2023-11-06_21-19-07/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m \r  0%|          | 0/170498071 [00:00<?, ?it/s]\n\u001B[36m(train_cifar pid=3952)\u001B[0m \r  1%|          | 884736/170498071 [00:00<00:20, 8172890.17it/s]\n\u001B[36m(train_cifar pid=3952)\u001B[0m \r  6%|▌         | 9568256/170498071 [00:00<00:03, 52899563.28it/s]\n\u001B[36m(train_cifar pid=3952)\u001B[0m \r 12%|█▏        | 20348928/170498071 [00:00<00:01, 77405448.14it/s]\n\u001B[36m(train_cifar pid=3952)\u001B[0m \r 18%|█▊        | 29949952/170498071 [00:00<00:01, 84600610.97it/s]\n\u001B[36m(train_cifar pid=3952)\u001B[0m \r 24%|██▍       | 40992768/170498071 [00:00<00:01, 93605270.70it/s]\n\u001B[36m(train_cifar pid=3952)\u001B[0m \r 30%|██▉       | 50429952/170498071 [00:00<00:01, 93687562.55it/s]\n\u001B[36m(train_cifar pid=3952)\u001B[0m \r 36%|███▌      | 61734912/170498071 [00:00<00:01, 99929381.80it/s]\n\u001B[36m(train_cifar pid=3952)\u001B[0m \r 42%|████▏     | 71761920/170498071 [00:00<00:01, 97541930.99it/s]\n\u001B[36m(train_cifar pid=3952)\u001B[0m \r 49%|████▊     | 83034112/170498071 [00:00<00:00, 101972135.19it/s]\n\u001B[36m(train_cifar pid=3952)\u001B[0m \r 55%|█████▍    | 93290496/170498071 [00:01<00:00, 99674322.88it/s] \n\u001B[36m(train_cifar pid=3952)\u001B[0m \r 61%|██████    | 104300544/170498071 [00:01<00:00, 102733221.21it/s]\n\u001B[36m(train_cifar pid=3952)\u001B[0m \r 67%|██████▋   | 114622464/170498071 [00:01<00:00, 100752070.69it/s]\n\u001B[36m(train_cifar pid=3952)\u001B[0m \r 74%|███████▎  | 125534208/170498071 [00:01<00:00, 103160129.46it/s]\n\u001B[36m(train_cifar pid=3952)\u001B[0m \r 80%|███████▉  | 135888896/170498071 [00:01<00:00, 101530127.20it/s]\n\u001B[36m(train_cifar pid=3952)\u001B[0m \r 86%|████████▌ | 146669568/170498071 [00:01<00:00, 103342423.19it/s]\n\u001B[36m(train_cifar pid=3952)\u001B[0m \r 92%|█████████▏| 157024256/170498071 [00:01<00:00, 101565607.18it/s]\n\u001B[36m(train_cifar pid=3952)\u001B[0m \r 98%|█████████▊| 167772160/170498071 [00:01<00:00, 103065866.01it/s]\r100%|██████████| 170498071/170498071 [00:01<00:00, 96119748.64it/s] \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m Extracting /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00000_0_batch_size=2,lr=0.0012_2023-11-06_21-19-07/data/cifar-10-python.tar.gz to /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00000_0_batch_size=2,lr=0.0012_2023-11-06_21-19-07/data\n\nTrial train_cifar_1e5c3_00002 started with configuration:\n+--------------------------------------------------+\n| Trial train_cifar_1e5c3_00002 config             |\n+--------------------------------------------------+\n| batch_size                                     4 |\n| l1                                             4 |\n| l2                                           128 |\n| lr                                       0.01267 |\n| max_epoch                                     20 |\n+--------------------------------------------------+\n\nTrial train_cifar_1e5c3_00003 started with configuration:\n+--------------------------------------------------+\n| Trial train_cifar_1e5c3_00003 config             |\n+--------------------------------------------------+\n| batch_size                                     8 |\n| l1                                            32 |\n| l2                                             4 |\n| lr                                       0.00612 |\n| max_epoch                                     20 |\n+--------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m num_cpus: 1\n\u001B[36m(train_cifar pid=3952)\u001B[0m Files already downloaded and verified\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m /databricks/python/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n\u001B[36m(train_cifar pid=3952)\u001B[0m   warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00002_2_batch_size=4,lr=0.0127_2023-11-06_21-19-07/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r  0%|          | 0/170498071 [00:00<?, ?it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r  0%|          | 851968/170498071 [00:00<00:21, 8014472.36it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r  6%|▋         | 10813440/170498071 [00:00<00:02, 60476090.13it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 13%|█▎        | 22544384/170498071 [00:00<00:01, 85989520.05it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 20%|██        | 34275328/170498071 [00:00<00:01, 98163341.69it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 27%|██▋       | 45940736/170498071 [00:00<00:01, 104673581.07it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 34%|███▎      | 57442304/170498071 [00:00<00:01, 108095831.64it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 40%|████      | 68517888/170498071 [00:00<00:00, 108887165.64it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 47%|████▋     | 79462400/170498071 [00:00<00:00, 105988804.54it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 53%|█████▎    | 90112000/170498071 [00:00<00:00, 103784804.37it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 59%|█████▉    | 100532224/170498071 [00:01<00:00, 101536681.78it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 65%|██████▍   | 110723072/170498071 [00:01<00:00, 99394682.39it/s] \n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 71%|███████   | 120684544/170498071 [00:01<00:00, 97347972.21it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 77%|███████▋  | 130449408/170498071 [00:01<00:00, 95138859.24it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 82%|████████▏ | 139984896/170498071 [00:01<00:00, 92814452.89it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 88%|████████▊ | 149291008/170498071 [00:01<00:00, 90359122.63it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 93%|█████████▎| 158367744/170498071 [00:01<00:00, 88898439.35it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 98%|█████████▊| 167280640/170498071 [00:01<00:00, 88443357.24it/s]\r100%|██████████| 170498071/170498071 [00:01<00:00, 93705414.39it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m Extracting /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00002_2_batch_size=4,lr=0.0127_2023-11-06_21-19-07/data/cifar-10-python.tar.gz to /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00002_2_batch_size=4,lr=0.0127_2023-11-06_21-19-07/data\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m Files already downloaded and verified\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m num_cpus: 1\n\u001B[36m(train_cifar pid=3953)\u001B[0m [1,  2000] loss: 2.164\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m /databricks/python/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m   warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00003_3_batch_size=8,lr=0.0061_2023-11-06_21-19-07/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r  0%|          | 0/170498071 [00:00<?, ?it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r  1%|          | 917504/170498071 [00:00<00:19, 8540464.71it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r  6%|▋         | 10715136/170498071 [00:00<00:02, 59536090.79it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 13%|█▎        | 21954560/170498071 [00:00<00:01, 83189601.93it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 20%|█▉        | 33685504/170498071 [00:00<00:01, 96449741.25it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 27%|██▋       | 45416448/170498071 [00:00<00:01, 103897763.09it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 33%|███▎      | 57081856/170498071 [00:00<00:01, 108199350.79it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 40%|████      | 68812800/170498071 [00:00<00:00, 111142980.04it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 47%|████▋     | 80510976/170498071 [00:00<00:00, 112966273.79it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 54%|█████▍    | 92176384/170498071 [00:00<00:00, 113974697.57it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 61%|██████    | 103940096/170498071 [00:01<00:00, 115011364.52it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 68%|██████▊   | 115605504/170498071 [00:01<00:00, 115412182.24it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 75%|███████▍  | 127205376/170498071 [00:01<00:00, 115576311.14it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 81%|████████▏ | 138936320/170498071 [00:01<00:00, 116034887.89it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 88%|████████▊ | 150568960/170498071 [00:01<00:00, 115971483.19it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 95%|█████████▌| 162168832/170498071 [00:01<00:00, 115380358.01it/s]\r100%|██████████| 170498071/170498071 [00:01<00:00, 107722982.24it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Extracting /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00003_3_batch_size=8,lr=0.0061_2023-11-06_21-19-07/data/cifar-10-python.tar.gz to /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00003_3_batch_size=8,lr=0.0061_2023-11-06_21-19-07/data\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000000)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTrial status: 4 RUNNING | 4 PENDING\nCurrent time: 2023-11-06 21:19:37. Total running time: 30s\nLogical resource usage: 4.0/8 CPUs, 2.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=2.004182459259033 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+-------------------------------------------------------------------------------------------------------------------+\n| Trial name                status             lr     batch_size     iter     total time (s)      loss     accuracy |\n+-------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING    0.00119966              2                                                    |\n| train_cifar_1e5c3_00001   RUNNING    0.00170084             16        1             23.004   2.00418       0.2544 |\n| train_cifar_1e5c3_00002   RUNNING    0.0126673               4                                                    |\n| train_cifar_1e5c3_00003   RUNNING    0.00612462              8                                                    |\n| train_cifar_1e5c3_00004   PENDING    0.0312312               4                                                    |\n| train_cifar_1e5c3_00005   PENDING    0.00117903              4                                                    |\n| train_cifar_1e5c3_00006   PENDING    0.0627759              16                                                    |\n| train_cifar_1e5c3_00007   PENDING    0.013142                8                                                    |\n+-------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Files already downloaded and verified\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [1,  2000] loss: 2.292\u001B[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001B[0m\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [1,  2000] loss: 2.006\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000001)\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m /databricks/python/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m   warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m [1,  6000] loss: 0.616\u001B[32m [repeated 3x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3953)\u001B[0m [3,  2000] loss: 1.582\u001B[32m [repeated 3x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00003_3_batch_size=8,lr=0.0061_2023-11-06_21-19-07/checkpoint_000000)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial status: 4 RUNNING | 4 PENDING\nCurrent time: 2023-11-06 21:20:07. Total running time: 1min 0s\nLogical resource usage: 4.0/8 CPUs, 2.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.5679220623016357 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+-------------------------------------------------------------------------------------------------------------------+\n| Trial name                status             lr     batch_size     iter     total time (s)      loss     accuracy |\n+-------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING    0.00119966              2                                                    |\n| train_cifar_1e5c3_00001   RUNNING    0.00170084             16        3            54.5407   1.56792       0.4043 |\n| train_cifar_1e5c3_00002   RUNNING    0.0126673               4                                                    |\n| train_cifar_1e5c3_00003   RUNNING    0.00612462              8        1            39.3462   1.62711       0.3775 |\n| train_cifar_1e5c3_00004   PENDING    0.0312312               4                                                    |\n| train_cifar_1e5c3_00005   PENDING    0.00117903              4                                                    |\n| train_cifar_1e5c3_00006   PENDING    0.0627759              16                                                    |\n| train_cifar_1e5c3_00007   PENDING    0.013142                8                                                    |\n+-------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [2,  2000] loss: 1.576\u001B[32m [repeated 3x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3953)\u001B[0m [4,  2000] loss: 1.492\u001B[32m [repeated 3x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00002_2_batch_size=4,lr=0.0127_2023-11-06_21-19-07/checkpoint_000000)\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m [1, 12000] loss: 0.280\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00003_3_batch_size=8,lr=0.0061_2023-11-06_21-19-07/checkpoint_000001)\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [2,  2000] loss: 2.219\n\u001B[36m(train_cifar pid=3952)\u001B[0m [1, 14000] loss: 0.232\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000004)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial status: 4 RUNNING | 4 PENDING\nCurrent time: 2023-11-06 21:20:37. Total running time: 1min 30s\nLogical resource usage: 4.0/8 CPUs, 2.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.450542694759369 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+-------------------------------------------------------------------------------------------------------------------+\n| Trial name                status             lr     batch_size     iter     total time (s)      loss     accuracy |\n+-------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING    0.00119966              2                                                    |\n| train_cifar_1e5c3_00001   RUNNING    0.00170084             16        5            85.975    1.45054       0.4546 |\n| train_cifar_1e5c3_00002   RUNNING    0.0126673               4        1            58.2827   2.20499       0.1308 |\n| train_cifar_1e5c3_00003   RUNNING    0.00612462              8        2            66.395    1.50894       0.4796 |\n| train_cifar_1e5c3_00004   PENDING    0.0312312               4                                                    |\n| train_cifar_1e5c3_00005   PENDING    0.00117903              4                                                    |\n| train_cifar_1e5c3_00006   PENDING    0.0627759              16                                                    |\n| train_cifar_1e5c3_00007   PENDING    0.013142                8                                                    |\n+-------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [3,  2000] loss: 1.485\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [2,  6000] loss: 0.740\u001B[32m [repeated 3x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000005)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [2,  8000] loss: 0.553\u001B[32m [repeated 4x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3953)\u001B[0m [7,  2000] loss: 1.370\u001B[32m [repeated 2x across cluster]\u001B[0m\nTrial status: 4 RUNNING | 4 PENDING\nCurrent time: 2023-11-06 21:21:07. Total running time: 2min 0s\nLogical resource usage: 4.0/8 CPUs, 2.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.3976775547981262 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+-------------------------------------------------------------------------------------------------------------------+\n| Trial name                status             lr     batch_size     iter     total time (s)      loss     accuracy |\n+-------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING    0.00119966              2                                                    |\n| train_cifar_1e5c3_00001   RUNNING    0.00170084             16        6           101.49     1.39768       0.4638 |\n| train_cifar_1e5c3_00002   RUNNING    0.0126673               4        1            58.2827   2.20499       0.1308 |\n| train_cifar_1e5c3_00003   RUNNING    0.00612462              8        3            93.2333   1.47641       0.4711 |\n| train_cifar_1e5c3_00004   PENDING    0.0312312               4                                                    |\n| train_cifar_1e5c3_00005   PENDING    0.00117903              4                                                    |\n| train_cifar_1e5c3_00006   PENDING    0.0627759              16                                                    |\n| train_cifar_1e5c3_00007   PENDING    0.013142                8                                                    |\n+-------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000006)\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [4,  4000] loss: 0.724\u001B[32m [repeated 3x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00000_0_batch_size=2,lr=0.0012_2023-11-06_21-19-07/checkpoint_000000)\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m [8,  2000] loss: 1.325\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [3,  2000] loss: 2.309\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00003_3_batch_size=8,lr=0.0061_2023-11-06_21-19-07/checkpoint_000003)\n\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000007)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [3,  4000] loss: 1.155\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3953)\u001B[0m [9,  2000] loss: 1.283\u001B[32m [repeated 3x across cluster]\u001B[0m\nTrial status: 4 RUNNING | 4 PENDING\nCurrent time: 2023-11-06 21:21:37. Total running time: 2min 30s\nLogical resource usage: 4.0/8 CPUs, 2.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.3287887833595275 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+-------------------------------------------------------------------------------------------------------------------+\n| Trial name                status             lr     batch_size     iter     total time (s)      loss     accuracy |\n+-------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING    0.00119966              2        1            123.353   1.54597       0.4237 |\n| train_cifar_1e5c3_00001   RUNNING    0.00170084             16        8            133.219   1.32879       0.5029 |\n| train_cifar_1e5c3_00002   RUNNING    0.0126673               4        2            109.408   2.30594       0.0998 |\n| train_cifar_1e5c3_00003   RUNNING    0.00612462              8        4            120.284   1.53433       0.462  |\n| train_cifar_1e5c3_00004   PENDING    0.0312312               4                                                    |\n| train_cifar_1e5c3_00005   PENDING    0.00117903              4                                                    |\n| train_cifar_1e5c3_00006   PENDING    0.0627759              16                                                    |\n| train_cifar_1e5c3_00007   PENDING    0.013142                8                                                    |\n+-------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000008)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [5,  4000] loss: 0.721\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [3,  8000] loss: 0.577\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00003_3_batch_size=8,lr=0.0061_2023-11-06_21-19-07/checkpoint_000004)\n\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000009)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [3, 10000] loss: 0.462\u001B[32m [repeated 3x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3952)\u001B[0m [2, 10000] loss: 0.307\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00002_2_batch_size=4,lr=0.0127_2023-11-06_21-19-07/checkpoint_000002)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial status: 4 RUNNING | 4 PENDING\nCurrent time: 2023-11-06 21:22:07. Total running time: 3min 0s\nLogical resource usage: 4.0/8 CPUs, 2.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2945023712158203 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+-------------------------------------------------------------------------------------------------------------------+\n| Trial name                status             lr     batch_size     iter     total time (s)      loss     accuracy |\n+-------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING    0.00119966              2        1            123.353   1.54597       0.4237 |\n| train_cifar_1e5c3_00001   RUNNING    0.00170084             16       10            164.148   1.2945        0.5205 |\n| train_cifar_1e5c3_00002   RUNNING    0.0126673               4        3            161.34    2.30758       0.1001 |\n| train_cifar_1e5c3_00003   RUNNING    0.00612462              8        5            147.532   1.42643       0.5062 |\n| train_cifar_1e5c3_00004   PENDING    0.0312312               4                                                    |\n| train_cifar_1e5c3_00005   PENDING    0.00117903              4                                                    |\n| train_cifar_1e5c3_00006   PENDING    0.0627759              16                                                    |\n| train_cifar_1e5c3_00007   PENDING    0.013142                8                                                    |\n+-------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [6,  4000] loss: 0.720\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000010)\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00003_3_batch_size=8,lr=0.0061_2023-11-06_21-19-07/checkpoint_000005)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m [2, 14000] loss: 0.217\u001B[32m [repeated 3x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000011)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [7,  2000] loss: 1.420\u001B[32m [repeated 3x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [7,  4000] loss: 0.719\u001B[32m [repeated 3x across cluster]\u001B[0m\nTrial status: 4 RUNNING | 4 PENDING\nCurrent time: 2023-11-06 21:22:37. Total running time: 3min 30s\nLogical resource usage: 4.0/8 CPUs, 2.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2525162158966066 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+-------------------------------------------------------------------------------------------------------------------+\n| Trial name                status             lr     batch_size     iter     total time (s)      loss     accuracy |\n+-------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING    0.00119966              2        1            123.353   1.54597       0.4237 |\n| train_cifar_1e5c3_00001   RUNNING    0.00170084             16       12            195.164   1.25252       0.5434 |\n| train_cifar_1e5c3_00002   RUNNING    0.0126673               4        3            161.34    2.30758       0.1001 |\n| train_cifar_1e5c3_00003   RUNNING    0.00612462              8        6            174.658   1.50187       0.4445 |\n| train_cifar_1e5c3_00004   PENDING    0.0312312               4                                                    |\n| train_cifar_1e5c3_00005   PENDING    0.00117903              4                                                    |\n| train_cifar_1e5c3_00006   PENDING    0.0627759              16                                                    |\n| train_cifar_1e5c3_00007   PENDING    0.013142                8                                                    |\n+-------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000012)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [4, 10000] loss: 0.462\u001B[32m [repeated 4x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3953)\u001B[0m [14,  2000] loss: 1.215\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00002_2_batch_size=4,lr=0.0127_2023-11-06_21-19-07/checkpoint_000003)\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [8,  4000] loss: 0.709\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00000_0_batch_size=2,lr=0.0012_2023-11-06_21-19-07/checkpoint_000001)\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial status: 4 RUNNING | 4 PENDING\nCurrent time: 2023-11-06 21:23:07. Total running time: 4min 0s\nLogical resource usage: 4.0/8 CPUs, 2.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.3108190949440002 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+-------------------------------------------------------------------------------------------------------------------+\n| Trial name                status             lr     batch_size     iter     total time (s)      loss     accuracy |\n+-------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING    0.00119966              2        2            234.037   1.48939       0.4562 |\n| train_cifar_1e5c3_00001   RUNNING    0.00170084             16       14            227.265   1.31082       0.5243 |\n| train_cifar_1e5c3_00002   RUNNING    0.0126673               4        4            213.316   2.30947       0.104  |\n| train_cifar_1e5c3_00003   RUNNING    0.00612462              8        7            201.778   1.58681       0.4373 |\n| train_cifar_1e5c3_00004   PENDING    0.0312312               4                                                    |\n| train_cifar_1e5c3_00005   PENDING    0.00117903              4                                                    |\n| train_cifar_1e5c3_00006   PENDING    0.0627759              16                                                    |\n| train_cifar_1e5c3_00007   PENDING    0.013142                8                                                    |\n+-------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=3953)\u001B[0m [15,  2000] loss: 1.204\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00003_3_batch_size=8,lr=0.0061_2023-11-06_21-19-07/checkpoint_000007)\n\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000014)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m [3,  2000] loss: 1.473\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [9,  2000] loss: 1.406\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [5,  6000] loss: 0.770\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000015)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [9,  4000] loss: 0.712\u001B[32m [repeated 3x across cluster]\u001B[0m\nTrial status: 4 RUNNING | 4 PENDING\nCurrent time: 2023-11-06 21:23:37. Total running time: 4min 30s\nLogical resource usage: 4.0/8 CPUs, 2.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2756760026931762 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+-------------------------------------------------------------------------------------------------------------------+\n| Trial name                status             lr     batch_size     iter     total time (s)      loss     accuracy |\n+-------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING    0.00119966              2        2            234.037   1.48939       0.4562 |\n| train_cifar_1e5c3_00001   RUNNING    0.00170084             16       16            258.451   1.27568       0.533  |\n| train_cifar_1e5c3_00002   RUNNING    0.0126673               4        4            213.316   2.30947       0.104  |\n| train_cifar_1e5c3_00003   RUNNING    0.00612462              8        8            229.163   1.43191       0.5117 |\n| train_cifar_1e5c3_00004   PENDING    0.0312312               4                                                    |\n| train_cifar_1e5c3_00005   PENDING    0.00117903              4                                                    |\n| train_cifar_1e5c3_00006   PENDING    0.0627759              16                                                    |\n| train_cifar_1e5c3_00007   PENDING    0.013142                8                                                    |\n+-------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00003_3_batch_size=8,lr=0.0061_2023-11-06_21-19-07/checkpoint_000008)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [5, 10000] loss: 0.462\u001B[32m [repeated 3x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000016)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTrial train_cifar_1e5c3_00002 completed after 5 iterations at 2023-11-06 21:23:47. Total running time: 4min 40s\n+------------------------------------------------------------+\n| Trial train_cifar_1e5c3_00002 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000004 |\n| time_this_iter_s                                  52.12899 |\n| time_total_s                                     265.44513 |\n| training_iteration                                       5 |\n| accuracy                                             0.095 |\n| loss                                               2.30968 |\n| try_gpu                                              False |\n+------------------------------------------------------------+\n\nTrial train_cifar_1e5c3_00004 started with configuration:\n+--------------------------------------------------+\n| Trial train_cifar_1e5c3_00004 config             |\n+--------------------------------------------------+\n| batch_size                                     4 |\n| l1                                            64 |\n| l2                                           256 |\n| lr                                       0.03123 |\n| max_epoch                                     20 |\n+--------------------------------------------------+\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m num_cpus: 1\n\u001B[36m(train_cifar pid=3952)\u001B[0m [3,  8000] loss: 0.368\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00002_2_batch_size=4,lr=0.0127_2023-11-06_21-19-07/checkpoint_000004)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00004_4_batch_size=4,lr=0.0312_2023-11-06_21-19-07/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r  0%|          | 0/170498071 [00:00<?, ?it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r  0%|          | 851968/170498071 [00:00<00:22, 7635220.07it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r  6%|▌         | 9961472/170498071 [00:00<00:02, 54470896.21it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 11%|█         | 18350080/170498071 [00:00<00:02, 67537373.73it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 17%|█▋        | 29556736/170498071 [00:00<00:01, 84732720.83it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 22%|██▏       | 38174720/170498071 [00:00<00:01, 84460721.46it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 27%|██▋       | 46727168/170498071 [00:00<00:01, 80873391.01it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 32%|███▏      | 54919168/170498071 [00:00<00:01, 66819032.92it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 36%|███▋      | 62029824/170498071 [00:00<00:01, 60250899.20it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 40%|████      | 68419584/170498071 [00:01<00:01, 59391335.02it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 44%|████▎     | 74579968/170498071 [00:01<00:01, 55602322.04it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 47%|████▋     | 80314368/170498071 [00:01<00:01, 56020724.14it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 50%|█████     | 86048768/170498071 [00:01<00:01, 52986918.47it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 54%|█████▍    | 91717632/170498071 [00:01<00:01, 53874938.65it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 57%|█████▋    | 97189888/170498071 [00:01<00:01, 51349024.54it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 60%|██████    | 102760448/170498071 [00:01<00:01, 52392641.82it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 63%|██████▎   | 108068864/170498071 [00:01<00:01, 49534912.53it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 66%|██████▋   | 113082368/170498071 [00:01<00:01, 48195871.09it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 69%|██████▉   | 117964800/170498071 [00:02<00:01, 46999300.68it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 72%|███████▏  | 123174912/170498071 [00:02<00:00, 48178436.71it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 75%|███████▌  | 128024576/170498071 [00:02<00:00, 46219039.18it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 78%|███████▊  | 133070848/170498071 [00:02<00:00, 47205070.04it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 81%|████████  | 137822208/170498071 [00:02<00:00, 46401433.15it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 84%|████████▍ | 143294464/170498071 [00:02<00:00, 48735345.43it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 87%|████████▋ | 148209664/170498071 [00:02<00:00, 48229980.42it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 90%|█████████ | 153878528/170498071 [00:02<00:00, 50497641.84it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 93%|█████████▎| 158957568/170498071 [00:02<00:00, 48928649.20it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r 97%|█████████▋| 164626432/170498071 [00:03<00:00, 51131420.80it/s]\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m \r100%|█████████▉| 169771008/170498071 [00:03<00:00, 48700358.41it/s]\r100%|██████████| 170498071/170498071 [00:03<00:00, 54087254.04it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m Extracting /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00004_4_batch_size=4,lr=0.0312_2023-11-06_21-19-07/data/cifar-10-python.tar.gz to /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00004_4_batch_size=4,lr=0.0312_2023-11-06_21-19-07/data\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m Files already downloaded and verified\n\u001B[36m(train_cifar pid=3952)\u001B[0m [3, 10000] loss: 0.295\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000017)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m [3, 12000] loss: 0.240\u001B[32m [repeated 3x across cluster]\u001B[0m\n\nTrial train_cifar_1e5c3_00003 completed after 10 iterations at 2023-11-06 21:24:05. Total running time: 4min 57s\n+------------------------------------------------------------+\n| Trial train_cifar_1e5c3_00003 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000009 |\n| time_this_iter_s                                   27.0152 |\n| time_total_s                                      283.1032 |\n| training_iteration                                      10 |\n| accuracy                                            0.4804 |\n| loss                                               1.55756 |\n| try_gpu                                              False |\n+------------------------------------------------------------+\n\nTrial train_cifar_1e5c3_00005 started with configuration:\n+--------------------------------------------------+\n| Trial train_cifar_1e5c3_00005 config             |\n+--------------------------------------------------+\n| batch_size                                     4 |\n| l1                                            16 |\n| l2                                             4 |\n| lr                                       0.00118 |\n| max_epoch                                     20 |\n+--------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m num_cpus: 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00003_3_batch_size=8,lr=0.0061_2023-11-06_21-19-07/checkpoint_000009)\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r  0%|          | 0/170498071 [00:00<?, ?it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r  1%|          | 917504/170498071 [00:00<00:20, 8372875.28it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r  6%|▌         | 10649600/170498071 [00:00<00:02, 58637099.70it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 11%|█▏        | 19234816/170498071 [00:00<00:02, 70775024.20it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 18%|█▊        | 30670848/170498071 [00:00<00:01, 87732771.11it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 23%|██▎       | 39550976/170498071 [00:00<00:01, 87565269.06it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 30%|██▉       | 51019776/170498071 [00:00<00:01, 96663426.41it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 36%|███▌      | 60751872/170498071 [00:00<00:01, 93793243.67it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 42%|████▏     | 72220672/170498071 [00:00<00:00, 100219700.57it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 48%|████▊     | 82313216/170498071 [00:00<00:00, 96365247.41it/s] \n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 55%|█████▌    | 93782016/170498071 [00:01<00:00, 101770573.34it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 61%|██████    | 104038400/170498071 [00:01<00:00, 97992558.25it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 68%|██████▊   | 115507200/170498071 [00:01<00:00, 102809677.17it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 74%|███████▍  | 125861888/170498071 [00:01<00:00, 98882182.58it/s] \n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 81%|████████  | 137330688/170498071 [00:01<00:00, 103339151.49it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 87%|████████▋ | 147750912/170498071 [00:01<00:00, 98954777.15it/s] \n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r 93%|█████████▎| 159252480/170498071 [00:01<00:00, 103494718.98it/s]\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m \r100%|█████████▉| 169705472/170498071 [00:01<00:00, 99081300.78it/s] \r100%|██████████| 170498071/170498071 [00:01<00:00, 94504598.85it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Extracting /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/data/cifar-10-python.tar.gz to /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/data\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [1,  2000] loss: 2.288\n\nTrial status: 4 RUNNING | 2 TERMINATED | 2 PENDING\nCurrent time: 2023-11-06 21:24:07. Total running time: 5min 0s\nLogical resource usage: 4.0/8 CPUs, 2.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2368483323097228 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        2            234.037   1.48939       0.4562 |\n| train_cifar_1e5c3_00001   RUNNING      0.00170084             16       18            289.322   1.23685       0.5522 |\n| train_cifar_1e5c3_00004   RUNNING      0.0312312               4                                                    |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4                                                    |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5            265.445   2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10            283.103   1.55756       0.4804 |\n| train_cifar_1e5c3_00006   PENDING      0.0627759              16                                                    |\n| train_cifar_1e5c3_00007   PENDING      0.013142                8                                                    |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Files already downloaded and verified\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [1,  4000] loss: 1.127\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000018)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [1,  2000] loss: 2.211\u001B[32m [repeated 3x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3953)\u001B[0m [20,  2000] loss: 1.171\u001B[32m [repeated 3x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00001_1_batch_size=16,lr=0.0017_2023-11-06_21-19-07/checkpoint_000019)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTrial train_cifar_1e5c3_00001 completed after 20 iterations at 2023-11-06 21:24:32. Total running time: 5min 25s\n+------------------------------------------------------------+\n| Trial train_cifar_1e5c3_00001 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000019 |\n| time_this_iter_s                                  15.99379 |\n| time_total_s                                     321.87896 |\n| training_iteration                                      20 |\n| accuracy                                            0.5481 |\n| loss                                                1.2401 |\n| try_gpu                                              False |\n+------------------------------------------------------------+\n\nTrial train_cifar_1e5c3_00006 started with configuration:\n+--------------------------------------------------+\n| Trial train_cifar_1e5c3_00006 config             |\n+--------------------------------------------------+\n| batch_size                                    16 |\n| l1                                            64 |\n| l2                                            64 |\n| lr                                       0.06278 |\n| max_epoch                                     20 |\n+--------------------------------------------------+\n\u001B[36m(train_cifar pid=3953)\u001B[0m num_cpus: 1\n\u001B[36m(train_cifar pid=3952)\u001B[0m [3, 18000] loss: 0.161\u001B[32m [repeated 3x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3953)\u001B[0m Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00006_6_batch_size=16,lr=0.0628_2023-11-06_21-19-07/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m \r  0%|          | 0/170498071 [00:00<?, ?it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r  1%|          | 884736/170498071 [00:00<00:21, 8044736.12it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r  6%|▋         | 10747904/170498071 [00:00<00:02, 59187025.99it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 13%|█▎        | 22380544/170498071 [00:00<00:01, 84696253.11it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 20%|█▉        | 33947648/170498071 [00:00<00:01, 96728829.65it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 27%|██▋       | 45318144/170498071 [00:00<00:01, 102777836.25it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 33%|███▎      | 56918016/170498071 [00:00<00:01, 107136279.31it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 40%|████      | 68616192/170498071 [00:00<00:00, 109970022.52it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 47%|████▋     | 80445440/170498071 [00:00<00:00, 112572266.77it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 54%|█████▍    | 92143616/170498071 [00:00<00:00, 113913377.83it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 61%|██████    | 103841792/170498071 [00:01<00:00, 114856276.17it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 68%|██████▊   | 115539968/170498071 [00:01<00:00, 115460492.36it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 75%|███████▍  | 127270912/170498071 [00:01<00:00, 115912425.18it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 82%|████████▏ | 138969088/170498071 [00:01<00:00, 116204885.62it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 88%|████████▊ | 150667264/170498071 [00:01<00:00, 116395285.21it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 95%|█████████▌| 162365440/170498071 [00:01<00:00, 116523280.51it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r100%|██████████| 170498071/170498071 [00:01<00:00, 107746516.49it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Extracting /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00006_6_batch_size=16,lr=0.0628_2023-11-06_21-19-07/data/cifar-10-python.tar.gz to /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00006_6_batch_size=16,lr=0.0628_2023-11-06_21-19-07/data\n\u001B[36m(train_cifar pid=3953)\u001B[0m Files already downloaded and verified\n\nTrial status: 4 RUNNING | 3 TERMINATED | 1 PENDING\nCurrent time: 2023-11-06 21:24:38. Total running time: 5min 30s\nLogical resource usage: 4.0/8 CPUs, 2.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        2            234.037   1.48939       0.4562 |\n| train_cifar_1e5c3_00004   RUNNING      0.0312312               4                                                    |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4                                                    |\n| train_cifar_1e5c3_00006   RUNNING      0.0627759              16                                                    |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20            321.879   1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5            265.445   2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10            283.103   1.55756       0.4804 |\n| train_cifar_1e5c3_00007   PENDING      0.013142                8                                                    |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=3952)\u001B[0m [3, 20000] loss: 0.143\u001B[32m [repeated 3x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00004_4_batch_size=4,lr=0.0312_2023-11-06_21-19-07/checkpoint_000000)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m [1,  2000] loss: 2.248\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00006_6_batch_size=16,lr=0.0628_2023-11-06_21-19-07/checkpoint_000000)\n\u001B[36m(train_cifar pid=3952)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00000_0_batch_size=2,lr=0.0012_2023-11-06_21-19-07/checkpoint_000002)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [2,  4000] loss: 1.160\u001B[32m [repeated 3x across cluster]\u001B[0m\nTrial status: 4 RUNNING | 3 TERMINATED | 1 PENDING\nCurrent time: 2023-11-06 21:25:08. Total running time: 6min 0s\nLogical resource usage: 4.0/8 CPUs, 2.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        3           345.506    1.46644       0.4744 |\n| train_cifar_1e5c3_00004   RUNNING      0.0312312               4        1            56.1369   2.3234        0.0985 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4        1            55.4678   1.75495       0.302  |\n| train_cifar_1e5c3_00006   RUNNING      0.0627759              16        1            20.8066   2.30895       0.0997 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00007   PENDING      0.013142                8                                                    |\n+---------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00006_6_batch_size=16,lr=0.0628_2023-11-06_21-19-07/checkpoint_000001)\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [2,  2000] loss: 1.752\u001B[32m [repeated 3x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3952)\u001B[0m [4,  4000] loss: 0.708\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00006_6_batch_size=16,lr=0.0628_2023-11-06_21-19-07/checkpoint_000002)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [2, 10000] loss: 0.464\u001B[32m [repeated 4x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00004_4_batch_size=4,lr=0.0312_2023-11-06_21-19-07/checkpoint_000001)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [2,  8000] loss: 0.427\u001B[32m [repeated 3x across cluster]\u001B[0m\nTrial status: 4 RUNNING | 3 TERMINATED | 1 PENDING\nCurrent time: 2023-11-06 21:25:38. Total running time: 6min 30s\nLogical resource usage: 4.0/8 CPUs, 2.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        3           345.506    1.46644       0.4744 |\n| train_cifar_1e5c3_00004   RUNNING      0.0312312               4        2           106.084    2.32781       0.1007 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4        1            55.4678   1.75495       0.302  |\n| train_cifar_1e5c3_00006   RUNNING      0.0627759              16        3            52.6653   2.30465       0.0993 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00007   PENDING      0.013142                8                                                    |\n+---------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00006_6_batch_size=16,lr=0.0628_2023-11-06_21-19-07/checkpoint_000003)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [3,  2000] loss: 2.321\u001B[32m [repeated 3x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [3,  4000] loss: 1.159\u001B[32m [repeated 3x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000001)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m [4, 12000] loss: 0.238\u001B[32m [repeated 2x across cluster]\u001B[0m\n\nTrial train_cifar_1e5c3_00006 completed after 5 iterations at 2023-11-06 21:25:59. Total running time: 6min 51s\n+------------------------------------------------------------+\n| Trial train_cifar_1e5c3_00006 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000004 |\n| time_this_iter_s                                  17.02978 |\n| time_total_s                                      86.46692 |\n| training_iteration                                       5 |\n| accuracy                                            0.1038 |\n| loss                                               2.31322 |\n| try_gpu                                              False |\n+------------------------------------------------------------+\n\nTrial train_cifar_1e5c3_00007 started with configuration:\n+--------------------------------------------------+\n| Trial train_cifar_1e5c3_00007 config             |\n+--------------------------------------------------+\n| batch_size                                     8 |\n| l1                                             4 |\n| l2                                           256 |\n| lr                                       0.01314 |\n| max_epoch                                     20 |\n+--------------------------------------------------+\n\u001B[36m(train_cifar pid=3953)\u001B[0m num_cpus: 1\n\u001B[36m(train_cifar pid=3953)\u001B[0m Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00007_7_batch_size=8,lr=0.0131_2023-11-06_21-19-07/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00006_6_batch_size=16,lr=0.0628_2023-11-06_21-19-07/checkpoint_000004)\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r  0%|          | 0/170498071 [00:00<?, ?it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r  1%|          | 917504/170498071 [00:00<00:20, 8204575.92it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r  6%|▋         | 10911744/170498071 [00:00<00:02, 59588343.32it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 13%|█▎        | 22577152/170498071 [00:00<00:01, 84933196.59it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 20%|██        | 34373632/170498071 [00:00<00:01, 97729509.33it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 27%|██▋       | 46104576/170498071 [00:00<00:01, 104647190.23it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 34%|███▍      | 57835520/170498071 [00:00<00:01, 108917397.86it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 41%|████      | 69566464/170498071 [00:00<00:00, 111602454.04it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 48%|████▊     | 81199104/170498071 [00:00<00:00, 111518428.21it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 55%|█████▍    | 93519872/170498071 [00:00<00:00, 115123116.73it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 62%|██████▏   | 105283584/170498071 [00:01<00:00, 115772610.93it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 69%|██████▊   | 117014528/170498071 [00:01<00:00, 116208452.73it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 76%|███████▌  | 128745472/170498071 [00:01<00:00, 116045766.41it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 82%|████████▏ | 140640256/170498071 [00:01<00:00, 116863352.27it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 89%|████████▉ | 152371200/170498071 [00:01<00:00, 116993963.20it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r 96%|█████████▌| 164102144/170498071 [00:01<00:00, 117070157.46it/s]\n\u001B[36m(train_cifar pid=3953)\u001B[0m \r100%|██████████| 170498071/170498071 [00:01<00:00, 108321674.87it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(autoscaler +7m24s)\u001B[0m Resized to 12 CPUs, 3 GPUs.\n\u001B[36m(train_cifar pid=3953)\u001B[0m Extracting /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00007_7_batch_size=8,lr=0.0131_2023-11-06_21-19-07/data/cifar-10-python.tar.gz to /root/ray_results/tune_checkpointing_location/train_cifar_1e5c3_00007_7_batch_size=8,lr=0.0131_2023-11-06_21-19-07/data\n\u001B[36m(autoscaler +7m25s)\u001B[0m Resized to 16 CPUs, 4 GPUs.\n\u001B[36m(train_cifar pid=3953)\u001B[0m Files already downloaded and verified\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [3,  2000] loss: 1.647\u001B[32m [repeated 2x across cluster]\u001B[0m\n\nTrial status: 4 RUNNING | 4 TERMINATED\nCurrent time: 2023-11-06 21:26:08. Total running time: 7min 0s\nLogical resource usage: 4.0/8 CPUs, 2.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        3           345.506    1.46644       0.4744 |\n| train_cifar_1e5c3_00004   RUNNING      0.0312312               4        2           106.084    2.32781       0.1007 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4        2           105.909    1.63359       0.3558 |\n| train_cifar_1e5c3_00007   RUNNING      0.013142                8                                                    |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=3953)\u001B[0m [1,  2000] loss: 2.138\u001B[32m [repeated 4x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00004_4_batch_size=4,lr=0.0312_2023-11-06_21-19-07/checkpoint_000002)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m [1,  4000] loss: 1.051\u001B[32m [repeated 4x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [4,  2000] loss: 2.322\u001B[32m [repeated 3x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00007_7_batch_size=8,lr=0.0131_2023-11-06_21-19-07/checkpoint_000000)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial status: 4 RUNNING | 4 TERMINATED\nCurrent time: 2023-11-06 21:26:38. Total running time: 7min 30s\nLogical resource usage: 4.0/16 CPUs, 2.0/4 GPUs (0.0/6.0 NODE_ID_AS_RESOURCE, 0.0/4.0 accelerator_type:V100)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        3           345.506    1.46644       0.4744 |\n| train_cifar_1e5c3_00004   RUNNING      0.0312312               4        3           155.782    2.32546       0.0973 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4        2           105.909    1.63359       0.3558 |\n| train_cifar_1e5c3_00007   RUNNING      0.013142                8        1            35.3372   2.30483       0.1026 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [4,  4000] loss: 1.160\u001B[32m [repeated 3x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000002)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [4,  6000] loss: 0.773\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00000_0_batch_size=2,lr=0.0012_2023-11-06_21-19-07/checkpoint_000003)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m [2,  4000] loss: 1.145\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 21:27:01,548\tWARNING spark_job_server.py:69 -- Spark job ray-cluster-9124-f5123551-worker-node-2 hosting Ray worker node exit.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(autoscaler +8m25s)\u001B[0m Removing 1 nodes of type ray.worker (idle).\n\u001B[36m(train_cifar pid=3952)\u001B[0m [5,  2000] loss: 1.402\u001B[32m [repeated 3x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 21:27:02,546\tWARNING spark_job_server.py:69 -- Spark job ray-cluster-9124-f5123551-worker-node-3 hosting Ray worker node exit.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(autoscaler +8m26s)\u001B[0m Removing 1 nodes of type ray.worker (idle).\n\u001B[36m(autoscaler +8m27s)\u001B[0m Resized to 12 CPUs, 3 GPUs.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00007_7_batch_size=8,lr=0.0131_2023-11-06_21-19-07/checkpoint_000001)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(autoscaler +8m28s)\u001B[0m Resized to 8 CPUs, 2 GPUs.\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [4,  6000] loss: 0.529\u001B[32m [repeated 2x across cluster]\u001B[0m\nTrial status: 4 RUNNING | 4 TERMINATED\nCurrent time: 2023-11-06 21:27:08. Total running time: 8min 0s\nLogical resource usage: 4.0/16 CPUs, 2.0/4 GPUs (0.0/6.0 NODE_ID_AS_RESOURCE, 0.0/4.0 accelerator_type:V100)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        4           461.557    1.51442       0.4371 |\n| train_cifar_1e5c3_00004   RUNNING      0.0312312               4        3           155.782    2.32546       0.0973 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4        3           156.417    1.59299       0.3683 |\n| train_cifar_1e5c3_00007   RUNNING      0.013142                8        2            65.0634   2.11248       0.1728 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n+---------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00004_4_batch_size=4,lr=0.0312_2023-11-06_21-19-07/checkpoint_000003)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m [3,  2000] loss: 2.117\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3952)\u001B[0m [5,  6000] loss: 0.463\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3952)\u001B[0m [5,  8000] loss: 0.354\u001B[32m [repeated 4x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000003)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial status: 4 RUNNING | 4 TERMINATED\nCurrent time: 2023-11-06 21:27:38. Total running time: 8min 30s\nLogical resource usage: 4.0/8 CPUs, 2.0/2 GPUs (0.0/1.0 NODE_ID_AS_RESOURCE, 0.0/2.0 accelerator_type:V100)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        4           461.557    1.51442       0.4371 |\n| train_cifar_1e5c3_00004   RUNNING      0.0312312               4        4           205.749    2.31443       0.0985 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4        4           207.434    1.61514       0.3952 |\n| train_cifar_1e5c3_00007   RUNNING      0.013142                8        3            93.5607   2.13533       0.159  |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [5,  6000] loss: 0.773\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [5,  8000] loss: 0.580\u001B[32m [repeated 4x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2848, ip=10.139.64.113)\u001B[0m [5, 10000] loss: 0.465\u001B[32m [repeated 4x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00007_7_batch_size=8,lr=0.0131_2023-11-06_21-19-07/checkpoint_000003)\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTrial train_cifar_1e5c3_00004 completed after 5 iterations at 2023-11-06 21:28:03. Total running time: 8min 56s\n+------------------------------------------------------------+\n| Trial train_cifar_1e5c3_00004 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000004 |\n| time_this_iter_s                                  50.35731 |\n| time_total_s                                     256.10656 |\n| training_iteration                                       5 |\n| accuracy                                            0.0964 |\n| loss                                               2.31252 |\n| try_gpu                                              False |\n+------------------------------------------------------------+\n\u001B[36m(train_cifar pid=3952)\u001B[0m [5, 16000] loss: 0.178\u001B[32m [repeated 3x across cluster]\u001B[0m\n\nTrial status: 3 RUNNING | 5 TERMINATED\nCurrent time: 2023-11-06 21:28:08. Total running time: 9min 0s\nLogical resource usage: 3.0/8 CPUs, 1.5/2 GPUs (0.0/1.0 NODE_ID_AS_RESOURCE, 0.0/2.0 accelerator_type:V100)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        4           461.557    1.51442       0.4371 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4        4           207.434    1.61514       0.3952 |\n| train_cifar_1e5c3_00007   RUNNING      0.013142                8        4           122.05     2.21488       0.1518 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [5, 10000] loss: 0.306\u001B[32m [repeated 3x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000004)\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m [5,  4000] loss: 1.094\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [6,  2000] loss: 1.489\u001B[32m [repeated 2x across cluster]\u001B[0m\n\nTrial train_cifar_1e5c3_00007 completed after 5 iterations at 2023-11-06 21:28:30. Total running time: 9min 23s\n+------------------------------------------------------------+\n| Trial train_cifar_1e5c3_00007 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000004 |\n| time_this_iter_s                                  29.10241 |\n| time_total_s                                     151.15273 |\n| training_iteration                                       5 |\n| accuracy                                            0.1636 |\n| loss                                               2.16655 |\n| try_gpu                                              False |\n+------------------------------------------------------------+\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3953)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00007_7_batch_size=8,lr=0.0131_2023-11-06_21-19-07/checkpoint_000004)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [6,  4000] loss: 0.731\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00000_0_batch_size=2,lr=0.0012_2023-11-06_21-19-07/checkpoint_000004)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTrial status: 2 RUNNING | 6 TERMINATED\nCurrent time: 2023-11-06 21:28:38. Total running time: 9min 30s\nLogical resource usage: 2.0/8 CPUs, 1.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        5           566.968    1.45453       0.4871 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4        5           254.897    1.5355        0.4307 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [6,  6000] loss: 0.482\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [6,  8000] loss: 0.363\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [6, 10000] loss: 0.289\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000005)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial status: 2 RUNNING | 6 TERMINATED\nCurrent time: 2023-11-06 21:29:08. Total running time: 10min 0s\nLogical resource usage: 2.0/8 CPUs, 1.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        5           566.968    1.45453       0.4871 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4        6           298.101    1.49083       0.4642 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=3952)\u001B[0m [6,  8000] loss: 0.353\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [7,  4000] loss: 0.697\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [7,  6000] loss: 0.464\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [7,  8000] loss: 0.348\u001B[32m [repeated 2x across cluster]\u001B[0m\nTrial status: 2 RUNNING | 6 TERMINATED\nCurrent time: 2023-11-06 21:29:38. Total running time: 10min 31s\nLogical resource usage: 2.0/8 CPUs, 1.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        5           566.968    1.45453       0.4871 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4        6           298.101    1.49083       0.4642 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [7, 10000] loss: 0.280\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000006)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m [6, 18000] loss: 0.156\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3952)\u001B[0m [6, 20000] loss: 0.143\u001B[32m [repeated 2x across cluster]\u001B[0m\nTrial status: 2 RUNNING | 6 TERMINATED\nCurrent time: 2023-11-06 21:30:08. Total running time: 11min 1s\nLogical resource usage: 2.0/8 CPUs, 1.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        5           566.968    1.45453       0.4871 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4        7           341.694    1.41876       0.4961 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00000_0_batch_size=2,lr=0.0012_2023-11-06_21-19-07/checkpoint_000005)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [8,  6000] loss: 0.455\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [8,  8000] loss: 0.337\n\u001B[36m(train_cifar pid=3952)\u001B[0m [7,  2000] loss: 1.356\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [8, 10000] loss: 0.271\n\u001B[36m(train_cifar pid=3952)\u001B[0m [7,  4000] loss: 0.687\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000007)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m [7,  6000] loss: 0.457\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [9,  2000] loss: 1.315\nTrial status: 2 RUNNING | 6 TERMINATED\nCurrent time: 2023-11-06 21:30:38. Total running time: 11min 31s\nLogical resource usage: 2.0/8 CPUs, 1.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        6           658.586    1.49881       0.4675 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4        8           384.953    1.40261       0.5072 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [9,  4000] loss: 0.658\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [9,  6000] loss: 0.443\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [9,  8000] loss: 0.334\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [9, 10000] loss: 0.268\u001B[32m [repeated 2x across cluster]\u001B[0m\nTrial status: 2 RUNNING | 6 TERMINATED\nCurrent time: 2023-11-06 21:31:08. Total running time: 12min 1s\nLogical resource usage: 2.0/8 CPUs, 1.0/2 GPUs (0.0/1.0 NODE_ID_AS_RESOURCE, 0.0/2.0 accelerator_type:V100)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        6           658.586    1.49881       0.4675 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4        8           384.953    1.40261       0.5072 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000008)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m [7, 18000] loss: 0.158\n\u001B[36m(train_cifar pid=3952)\u001B[0m [7, 20000] loss: 0.137\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00000_0_batch_size=2,lr=0.0012_2023-11-06_21-19-07/checkpoint_000006)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [10,  6000] loss: 0.443\u001B[32m [repeated 2x across cluster]\u001B[0m\nTrial status: 2 RUNNING | 6 TERMINATED\nCurrent time: 2023-11-06 21:31:38. Total running time: 12min 31s\nLogical resource usage: 2.0/8 CPUs, 1.0/2 GPUs (0.0/1.0 NODE_ID_AS_RESOURCE, 0.0/2.0 accelerator_type:V100)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        7           745.31     1.42207       0.491  |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4        9           428.3      1.47161       0.4837 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [10,  8000] loss: 0.324\n\u001B[36m(train_cifar pid=3952)\u001B[0m [8,  2000] loss: 1.351\n\u001B[36m(train_cifar pid=3952)\u001B[0m [8,  4000] loss: 0.686\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [10, 10000] loss: 0.264\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000009)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m [8,  6000] loss: 0.455\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [11,  2000] loss: 1.274\nTrial status: 2 RUNNING | 6 TERMINATED\nCurrent time: 2023-11-06 21:32:08. Total running time: 13min 1s\nLogical resource usage: 2.0/8 CPUs, 1.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        7           745.31     1.42207       0.491  |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4       10           471.904    1.35228       0.528  |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [11,  4000] loss: 0.646\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [11,  6000] loss: 0.427\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [11,  8000] loss: 0.322\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [11, 10000] loss: 0.262\u001B[32m [repeated 2x across cluster]\u001B[0m\nTrial status: 2 RUNNING | 6 TERMINATED\nCurrent time: 2023-11-06 21:32:38. Total running time: 13min 31s\nLogical resource usage: 2.0/8 CPUs, 1.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        7           745.31     1.42207       0.491  |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4       10           471.904    1.35228       0.528  |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000010)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m [8, 18000] loss: 0.151\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [12,  2000] loss: 1.224\n\u001B[36m(train_cifar pid=3952)\u001B[0m [8, 20000] loss: 0.142\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [12,  4000] loss: 0.640\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00000_0_batch_size=2,lr=0.0012_2023-11-06_21-19-07/checkpoint_000007)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [12,  6000] loss: 0.426\nTrial status: 2 RUNNING | 6 TERMINATED\nCurrent time: 2023-11-06 21:33:08. Total running time: 14min 1s\nLogical resource usage: 2.0/8 CPUs, 1.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        8           830.615    1.44869       0.4724 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4       11           515.681    1.34818       0.5307 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=3952)\u001B[0m [9,  2000] loss: 1.351\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [12,  8000] loss: 0.322\n\u001B[36m(train_cifar pid=3952)\u001B[0m [9,  4000] loss: 0.683\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [12, 10000] loss: 0.254\n\u001B[36m(train_cifar pid=3952)\u001B[0m [9,  6000] loss: 0.457\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000011)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m [9,  8000] loss: 0.337\n\u001B[36m(train_cifar pid=3952)\u001B[0m [9, 10000] loss: 0.278\u001B[32m [repeated 2x across cluster]\u001B[0m\nTrial status: 2 RUNNING | 6 TERMINATED\nCurrent time: 2023-11-06 21:33:38. Total running time: 14min 31s\nLogical resource usage: 2.0/8 CPUs, 1.0/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        8           830.615    1.44869       0.4724 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4       12           559.087    1.37696       0.5184 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=3952)\u001B[0m [9, 12000] loss: 0.229\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3952)\u001B[0m [9, 14000] loss: 0.201\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3952)\u001B[0m [9, 16000] loss: 0.172\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3952)\u001B[0m [9, 18000] loss: 0.155\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000012)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial status: 2 RUNNING | 6 TERMINATED\nCurrent time: 2023-11-06 21:34:08. Total running time: 15min 1s\nLogical resource usage: 2.0/8 CPUs, 1.0/2 GPUs (0.0/1.0 NODE_ID_AS_RESOURCE, 0.0/2.0 accelerator_type:V100)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        8           830.615    1.44869       0.4724 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4       13           602.575    1.415         0.5135 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [14,  2000] loss: 1.233\n\u001B[36m(train_cifar pid=3952)\u001B[0m [9, 20000] loss: 0.138\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [14,  4000] loss: 0.627\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00000_0_batch_size=2,lr=0.0012_2023-11-06_21-19-07/checkpoint_000008)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [14,  6000] loss: 0.424\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [14,  8000] loss: 0.314\u001B[32m [repeated 2x across cluster]\u001B[0m\nTrial status: 2 RUNNING | 6 TERMINATED\nCurrent time: 2023-11-06 21:34:38. Total running time: 15min 31s\nLogical resource usage: 2.0/8 CPUs, 1.0/2 GPUs (0.0/1.0 NODE_ID_AS_RESOURCE, 0.0/2.0 accelerator_type:V100)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        9           916.434    1.37025       0.5169 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4       13           602.575    1.415         0.5135 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [14, 10000] loss: 0.251\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000013)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m [10,  8000] loss: 0.337\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3952)\u001B[0m [10, 10000] loss: 0.266\u001B[32m [repeated 2x across cluster]\u001B[0m\nTrial status: 2 RUNNING | 6 TERMINATED\nCurrent time: 2023-11-06 21:35:08. Total running time: 16min 1s\nLogical resource usage: 2.0/8 CPUs, 1.0/2 GPUs (0.0/1.0 NODE_ID_AS_RESOURCE, 0.0/2.0 accelerator_type:V100)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        9           916.434    1.37025       0.5169 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4       14           646.043    1.344         0.5444 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=3952)\u001B[0m [10, 12000] loss: 0.228\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3952)\u001B[0m [10, 14000] loss: 0.193\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3952)\u001B[0m [10, 16000] loss: 0.171\u001B[32m [repeated 2x across cluster]\u001B[0m\n\u001B[36m(train_cifar pid=3952)\u001B[0m [10, 18000] loss: 0.149\u001B[32m [repeated 2x across cluster]\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000014)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial status: 2 RUNNING | 6 TERMINATED\nCurrent time: 2023-11-06 21:35:39. Total running time: 16min 31s\nLogical resource usage: 2.0/8 CPUs, 1.0/2 GPUs (0.0/1.0 NODE_ID_AS_RESOURCE, 0.0/2.0 accelerator_type:V100)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   RUNNING      0.00119966              2        9           916.434    1.37025       0.5169 |\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4       15           689.639    1.46727       0.5215 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=3952)\u001B[0m [10, 20000] loss: 0.138\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [16,  2000] loss: 1.212\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [16,  4000] loss: 0.608\n\nTrial train_cifar_1e5c3_00000 completed after 10 iterations at 2023-11-06 21:35:51. Total running time: 16min 44s\n+------------------------------------------------------------+\n| Trial train_cifar_1e5c3_00000 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000009 |\n| time_this_iter_s                                  84.70705 |\n| time_total_s                                      1001.141 |\n| training_iteration                                      10 |\n| accuracy                                            0.4933 |\n| loss                                               1.45024 |\n| try_gpu                                              False |\n+------------------------------------------------------------+\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=3952)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00000_0_batch_size=2,lr=0.0012_2023-11-06_21-19-07/checkpoint_000009)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [16,  6000] loss: 0.409\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [16,  8000] loss: 0.314\n\nTrial status: 7 TERMINATED | 1 RUNNING\nCurrent time: 2023-11-06 21:36:09. Total running time: 17min 1s\nLogical resource usage: 1.0/8 CPUs, 0.5/2 GPUs (0.0/1.0 NODE_ID_AS_RESOURCE, 0.0/2.0 accelerator_type:V100)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4       15           689.639    1.46727       0.5215 |\n| train_cifar_1e5c3_00000   TERMINATED   0.00119966              2       10          1001.14     1.45024       0.4933 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [16, 10000] loss: 0.249\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000015)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [17,  2000] loss: 1.187\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [17,  4000] loss: 0.598\nTrial status: 7 TERMINATED | 1 RUNNING\nCurrent time: 2023-11-06 21:36:39. Total running time: 17min 31s\nLogical resource usage: 1.0/8 CPUs, 0.5/2 GPUs (0.0/1.0 NODE_ID_AS_RESOURCE, 0.0/2.0 accelerator_type:V100)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4       16           733.409    1.33357       0.5445 |\n| train_cifar_1e5c3_00000   TERMINATED   0.00119966              2       10          1001.14     1.45024       0.4933 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [17,  6000] loss: 0.412\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [17,  8000] loss: 0.313\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [17, 10000] loss: 0.248\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000016)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial status: 7 TERMINATED | 1 RUNNING\nCurrent time: 2023-11-06 21:37:09. Total running time: 18min 1s\nLogical resource usage: 1.0/8 CPUs, 0.5/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4       17           777.155    1.40678       0.525  |\n| train_cifar_1e5c3_00000   TERMINATED   0.00119966              2       10          1001.14     1.45024       0.4933 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [18,  2000] loss: 1.156\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [18,  4000] loss: 0.606\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [18,  6000] loss: 0.404\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [18,  8000] loss: 0.305\nTrial status: 7 TERMINATED | 1 RUNNING\nCurrent time: 2023-11-06 21:37:39. Total running time: 18min 31s\nLogical resource usage: 1.0/8 CPUs, 0.5/2 GPUs (0.0/1.0 NODE_ID_AS_RESOURCE, 0.0/2.0 accelerator_type:V100)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4       17           777.155    1.40678       0.525  |\n| train_cifar_1e5c3_00000   TERMINATED   0.00119966              2       10          1001.14     1.45024       0.4933 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [18, 10000] loss: 0.251\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000017)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [19,  2000] loss: 1.192\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [19,  4000] loss: 0.597\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [19,  6000] loss: 0.400\nTrial status: 7 TERMINATED | 1 RUNNING\nCurrent time: 2023-11-06 21:38:09. Total running time: 19min 2s\nLogical resource usage: 1.0/8 CPUs, 0.5/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4       18           820.812    1.4182        0.5235 |\n| train_cifar_1e5c3_00000   TERMINATED   0.00119966              2       10          1001.14     1.45024       0.4933 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [19,  8000] loss: 0.307\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [19, 10000] loss: 0.246\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000018)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [20,  2000] loss: 1.194\nTrial status: 7 TERMINATED | 1 RUNNING\nCurrent time: 2023-11-06 21:38:39. Total running time: 19min 32s\nLogical resource usage: 1.0/8 CPUs, 0.5/2 GPUs (0.0/1.0 NODE_ID_AS_RESOURCE, 0.0/2.0 accelerator_type:V100)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4       19           864.621    1.38908       0.5269 |\n| train_cifar_1e5c3_00000   TERMINATED   0.00119966              2       10          1001.14     1.45024       0.4933 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [20,  4000] loss: 0.595\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [20,  6000] loss: 0.403\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [20,  8000] loss: 0.303\n\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m [20, 10000] loss: 0.241\nTrial status: 7 TERMINATED | 1 RUNNING\nCurrent time: 2023-11-06 21:39:09. Total running time: 20min 2s\nLogical resource usage: 1.0/8 CPUs, 0.5/2 GPUs (0.0/2.0 accelerator_type:V100, 0.0/1.0 NODE_ID_AS_RESOURCE)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00005   RUNNING      0.00117903              4       19           864.621    1.38908       0.5269 |\n| train_cifar_1e5c3_00000   TERMINATED   0.00119966              2       10          1001.14     1.45024       0.4933 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\nTrial train_cifar_1e5c3_00005 completed after 20 iterations at 2023-11-06 21:39:13. Total running time: 20min 6s\n+------------------------------------------------------------+\n| Trial train_cifar_1e5c3_00005 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000019 |\n| time_this_iter_s                                  43.60395 |\n| time_total_s                                     908.22465 |\n| training_iteration                                      20 |\n| accuracy                                            0.5278 |\n| loss                                               1.40754 |\n| try_gpu                                              False |\n+------------------------------------------------------------+\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_cifar pid=2850, ip=10.139.64.113)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/dbfs/pj/ray/tune_checkpointing_location/train_cifar_1e5c3_00005_5_batch_size=4,lr=0.0012_2023-11-06_21-19-07/checkpoint_000019)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTrial status: 8 TERMINATED\nCurrent time: 2023-11-06 21:39:16. Total running time: 20min 8s\nLogical resource usage: 1.0/8 CPUs, 0.5/2 GPUs (0.0/1.0 NODE_ID_AS_RESOURCE, 0.0/2.0 accelerator_type:V100)\nCurrent best trial: 1e5c3_00001 with loss=1.2401041355133056 and params={'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\n+---------------------------------------------------------------------------------------------------------------------+\n| Trial name                status               lr     batch_size     iter     total time (s)      loss     accuracy |\n+---------------------------------------------------------------------------------------------------------------------+\n| train_cifar_1e5c3_00000   TERMINATED   0.00119966              2       10          1001.14     1.45024       0.4933 |\n| train_cifar_1e5c3_00001   TERMINATED   0.00170084             16       20           321.879    1.2401        0.5481 |\n| train_cifar_1e5c3_00002   TERMINATED   0.0126673               4        5           265.445    2.30968       0.095  |\n| train_cifar_1e5c3_00003   TERMINATED   0.00612462              8       10           283.103    1.55756       0.4804 |\n| train_cifar_1e5c3_00004   TERMINATED   0.0312312               4        5           256.107    2.31252       0.0964 |\n| train_cifar_1e5c3_00005   TERMINATED   0.00117903              4       20           908.225    1.40754       0.5278 |\n| train_cifar_1e5c3_00006   TERMINATED   0.0627759              16        5            86.4669   2.31322       0.1038 |\n| train_cifar_1e5c3_00007   TERMINATED   0.013142                8        5           151.153    2.16655       0.1636 |\n+---------------------------------------------------------------------------------------------------------------------+\n\nBest trial config: {'l1': 4, 'l2': 8, 'lr': 0.0017008417716143818, 'batch_size': 16, 'max_epoch': 20}\nBest trial final validation loss: 1.2401041355133056\nBest trial final validation accuracy: 0.5481\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96deb2337eb47d78cfb03f996d6f5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\nBest trial test set accuracy: 0.5479\n"
     ]
    }
   ],
   "source": [
    "# Run a GPU only Trial\n",
    "main(num_samples=8, max_num_epochs=10,grace_period=5,cpus_per_trial=1, gpus_per_trial=0.5 , loc = '/dbfs/pj/ray/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f1d5d41-3243-4165-a9bb-bc8586a28312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4429164495716192,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ray_autoscaling_example",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
